{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©tection de Somnolence du Conducteur - Exploration et Pr√©paration\n",
    "\n",
    "## Vision par Ordinateur et Deep Learning\n",
    "\n",
    "Ce notebook couvre:\n",
    "1. **Exploration des donn√©es** - Visualisation des datasets\n",
    "2. **Pr√©traitement** - Normalisation, augmentation\n",
    "3. **Extraction de features** - EAR, MAR, Landmarks\n",
    "4. **Pr√©paration des donn√©es** - Cr√©ation des jeux train/validation/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standards\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "# Configuration des visualisations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Ajout du path source\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Imports du projet\n",
    "from utils.preprocessing import ImagePreprocessor\n",
    "from utils.metrics import calculate_ear, calculate_mar, FatigueMetrics\n",
    "from detection.face_detector import FaceDetector\n",
    "from detection.landmark_extractor import LandmarkExtractor\n",
    "\n",
    "print(\"‚úì Imports r√©ussis\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement de la Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la configuration\n",
    "with open('../config.yaml', 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration du projet:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Nom: {config['project']['name']}\")\n",
    "print(f\"Version: {config['project']['version']}\")\n",
    "print(f\"\")\n",
    "print(\"Param√®tres de d√©tection:\")\n",
    "print(f\"  - Seuil EAR: {config['detection']['eye_aspect_ratio_threshold']}\")\n",
    "print(f\"  - Seuil MAR: {config['detection']['mouth_aspect_ratio_threshold']}\")\n",
    "print(f\"  - Seuil PERCLOS: {config['detection']['perclos_threshold']*100}%\")\n",
    "print(f\"\")\n",
    "print(\"Dimensions des images:\")\n",
    "print(f\"  - Yeux: {config['preprocessing']['image_size_eye']}\")\n",
    "print(f\"  - Bouche: {config['preprocessing']['image_size_yawn']}\")\n",
    "print(f\"  - Transfer Learning: {config['preprocessing']['image_size_transfer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration du Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Structure des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins des donn√©es\n",
    "DATA_RAW = '../data/raw'\n",
    "DATA_PROCESSED = '../data/processed'\n",
    "\n",
    "# Exploration de la structure\n",
    "def explore_directory(path, level=0):\n",
    "    \"\"\"Explore r√©cursivement un r√©pertoire.\"\"\"\n",
    "    items = []\n",
    "    if os.path.exists(path):\n",
    "        for item in os.listdir(path):\n",
    "            item_path = os.path.join(path, item)\n",
    "            indent = \"  \" * level\n",
    "            if os.path.isdir(item_path):\n",
    "                num_files = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])\n",
    "                print(f\"{indent}üìÅ {item}/ ({num_files} fichiers)\")\n",
    "                explore_directory(item_path, level + 1)\n",
    "            else:\n",
    "                items.append(item)\n",
    "    return items\n",
    "\n",
    "print(\"Structure des donn√©es brutes:\")\n",
    "print(\"=\"*50)\n",
    "if os.path.exists(DATA_RAW):\n",
    "    explore_directory(DATA_RAW)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Le dossier {DATA_RAW} n'existe pas encore.\")\n",
    "    print(\"Cr√©ez la structure ou t√©l√©chargez les datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 G√©n√©ration de Donn√©es Synth√©tiques (D√©monstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation de donn√©es synth√©tiques pour d√©monstration\n",
    "# Dans un vrai projet, vous utiliserez des datasets r√©els\n",
    "\n",
    "def generate_synthetic_eye_data(n_samples=1000, img_size=(48, 48)):\n",
    "    \"\"\"\n",
    "    G√©n√®re des donn√©es synth√©tiques d'yeux pour d√©monstration.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Image de base (niveaux de gris)\n",
    "        img = np.random.randint(0, 50, size=(*img_size, 1), dtype=np.uint8)\n",
    "        \n",
    "        # Classe: 0 = ouvert, 1 = ferm√©\n",
    "        label = np.random.randint(0, 2)\n",
    "        \n",
    "        if label == 0:\n",
    "            # ≈íil ouvert: deux cercles (iris + pupille)\n",
    "            center = (img_size[1]//2, img_size[0]//2)\n",
    "            cv2.circle(img, center, 15, (200,), -1)  # Iris\n",
    "            cv2.circle(img, center, 7, (50,), -1)    # Pupille\n",
    "        else:\n",
    "            # ≈íil ferm√©: ligne horizontale\n",
    "            y = img_size[0]//2\n",
    "            cv2.line(img, (10, y), (img_size[1]-10, y), (150,), 3)\n",
    "        \n",
    "        # Ajout de bruit\n",
    "        noise = np.random.randint(-20, 20, size=img.shape, dtype=np.int16)\n",
    "        img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# G√©n√©ration des donn√©es\n",
    "print(\"G√©n√©ration de donn√©es synth√©tiques...\")\n",
    "X_eyes, y_eyes = generate_synthetic_eye_data(n_samples=1000)\n",
    "\n",
    "print(f\"‚úì Donn√©es g√©n√©r√©es:\")\n",
    "print(f\"  - Forme: {X_eyes.shape}\")\n",
    "print(f\"  - Classes: {np.bincount(y_eyes)}\")\n",
    "print(f\"  - Distribution: {np.bincount(y_eyes)[0]} ouverts, {np.bincount(y_eyes)[1]} ferm√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualisation des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des √©chantillons\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# ≈íils ouverts\n",
    "open_indices = np.where(y_eyes == 0)[0][:5]\n",
    "for i, idx in enumerate(open_indices):\n",
    "    axes[0, i].imshow(X_eyes[idx].squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title(f'Ouvert #{idx}')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# ≈íils ferm√©s\n",
    "closed_indices = np.where(y_eyes == 1)[0][:5]\n",
    "for i, idx in enumerate(closed_indices):\n",
    "    axes[1, i].imshow(X_eyes[idx].squeeze(), cmap='gray')\n",
    "    axes[1, i].set_title(f'Ferm√© #{idx}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Yeux Ouverts', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Yeux Ferm√©s', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('√âchantillons de Donn√©es Synth√©tiques - Yeux', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/data_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Distribution des Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des classes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar plot\n",
    "classes = ['Ouvert (0)', 'Ferm√© (1)']\n",
    "counts = np.bincount(y_eyes)\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = axes[0].bar(classes, counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_ylabel('Nombre d''√©chantillons')\n",
    "axes[0].set_title('Distribution des Classes')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{count}\\n({count/len(y_eyes)*100:.1f}%)',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(counts, labels=classes, colors=colors, autopct='%1.1f%%',\n",
    "          startangle=90, explode=(0.02, 0.02))\n",
    "axes[1].set_title('R√©partition des Classes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©traitement des Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialisation du Pr√©processeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du pr√©processeur\n",
    "preprocessor = ImagePreprocessor(config_path='../config.yaml')\n",
    "\n",
    "print(\"Pr√©processeur initialis√©:\")\n",
    "print(f\"  - Taille yeux: {preprocessor.eye_size}\")\n",
    "print(f\"  - Taille bouche: {preprocessor.yawn_size}\")\n",
    "print(f\"  - M√©thode normalisation: {preprocessor.normalize_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 D√©monstration du Pipeline de Pr√©traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration sur un √©chantillon\n",
    "sample_idx = 0\n",
    "sample_image = X_eyes[sample_idx]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(sample_image.squeeze(), cmap='gray')\n",
    "axes[0, 0].set_title('Image Originale', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Normalisation simple\n",
    "normalized = sample_image.astype(np.float32) / 255.0\n",
    "axes[0, 1].imshow(normalized.squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "axes[0, 1].set_title('Normalis√©e [0,1]', fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Redimensionn√©e\n",
    "resized = preprocessor.resize_image(sample_image.squeeze(), (48, 48))\n",
    "axes[0, 2].imshow(resized, cmap='gray')\n",
    "axes[0, 2].set_title('Redimensionn√©e (48x48)', fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Pipeline complet\n",
    "preprocessed = preprocessor.preprocess_eye(sample_image)\n",
    "axes[1, 0].imshow(preprocessed.squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "axes[1, 0].set_title('Pipeline Complet', fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Data Augmentation\n",
    "augmented = preprocessor.apply_augmentation(sample_image)\n",
    "axes[1, 1].imshow(augmented.squeeze(), cmap='gray')\n",
    "axes[1, 1].set_title('Augment√©e', fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Histogramme des pixels\n",
    "axes[1, 2].hist(sample_image.ravel(), bins=50, color='blue', alpha=0.7, label='Original')\n",
    "axes[1, 2].hist((preprocessed * 255).ravel(), bins=50, color='red', alpha=0.5, label='Normalis√©e')\n",
    "axes[1, 2].set_xlabel('Valeur des pixels')\n",
    "axes[1, 2].set_ylabel('Fr√©quence')\n",
    "axes[1, 2].set_title('Distribution des Pixels', fontweight='bold')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.suptitle('Pipeline de Pr√©traitement', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/preprocessing_pipeline.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Forme originale: {sample_image.shape}\")\n",
    "print(f\"Forme pr√©trait√©e: {preprocessed.shape}\")\n",
    "print(f\"Valeurs min/max: [{preprocessed.min():.3f}, {preprocessed.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extraction des M√©triques (EAR, MAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 D√©finition et Calcul de l'EAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©monstration du calcul EAR\n",
    "\n",
    "# Landmarks d'un ≈ìil ouvert (forme d'amande)\n",
    "eye_open_landmarks = [\n",
    "    (30, 25),   # P1: coin externe\n",
    "    (25, 20),   # P2: haut gauche\n",
    "    (35, 20),   # P3: haut droit\n",
    "    (30, 35),   # P4: coin interne\n",
    "    (25, 40),   # P5: bas gauche\n",
    "    (35, 40),   # P6: bas droit\n",
    "]\n",
    "\n",
    "# Landmarks d'un ≈ìil ferm√© (ligne)\n",
    "eye_closed_landmarks = [\n",
    "    (30, 30),   # P1\n",
    "    (28, 28),   # P2\n",
    "    (32, 28),   # P3\n",
    "    (30, 30),   # P4\n",
    "    (28, 32),   # P5\n",
    "    (32, 32),   # P6\n",
    "]\n",
    "\n",
    "# Calcul des EAR\n",
    "ear_open = calculate_ear(eye_open_landmarks)\n",
    "ear_closed = calculate_ear(eye_closed_landmarks)\n",
    "\n",
    "print(\"Calcul de l'Eye Aspect Ratio (EAR):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"EAR ≈ìil ouvert: {ear_open:.3f}\")\n",
    "print(f\"EAR ≈ìil ferm√©: {ear_closed:.3f}\")\n",
    "print(f\"Diff√©rence: {ear_open - ear_closed:.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"Seuil typique: ~0.25\")\n",
    "print(f\"Si EAR < 0.25 ‚Üí ≈íil consid√©r√© comme ferm√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualisation de l'EAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des landmarks et calcul EAR\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "def plot_eye_with_landmarks(ax, landmarks, title, ear_value):\n",
    "    \"\"\"Dessine un ≈ìil avec ses landmarks.\"\"\"\n",
    "    # Fond\n",
    "    ax.set_xlim(15, 45)\n",
    "    ax.set_ylim(45, 15)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_facecolor('#f0f0f0')\n",
    "    \n",
    "    # Points\n",
    "    xs = [p[0] for p in landmarks]\n",
    "    ys = [p[1] for p in landmarks]\n",
    "    \n",
    "    # Dessiner les connexions\n",
    "    # Ligne horizontale\n",
    "    ax.plot([landmarks[0][0], landmarks[3][0]], \n",
    "           [landmarks[0][1], landmarks[3][1]], \n",
    "           'b-', linewidth=2, label='Horizontal')\n",
    "    # Lignes verticales\n",
    "    ax.plot([landmarks[1][0], landmarks[5][0]], \n",
    "           [landmarks[1][1], landmarks[5][1]], \n",
    "           'r--', linewidth=2, label='Vertical 1')\n",
    "    ax.plot([landmarks[2][0], landmarks[4][0]], \n",
    "           [landmarks[2][1], landmarks[4][1]], \n",
    "           'r--', linewidth=2, label='Vertical 2')\n",
    "    \n",
    "    # Points\n",
    "    for i, (x, y) in enumerate(landmarks):\n",
    "        color = 'green' if i in [0, 3] else 'red'\n",
    "        ax.plot(x, y, 'o', markersize=10, color=color)\n",
    "        ax.annotate(f'P{i+1}', (x, y), textcoords=\"offset points\", \n",
    "                   xytext=(5, 5), fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f'{title}\\nEAR = {ear_value:.3f}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plot_eye_with_landmarks(axes[0], eye_open_landmarks, '≈íIL OUVERT', ear_open)\n",
    "plot_eye_with_landmarks(axes[1], eye_closed_landmarks, '≈íIL FERM√â', ear_closed)\n",
    "\n",
    "plt.suptitle('Calcul de l''Eye Aspect Ratio (EAR)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/ear_calculation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Simulation Temps R√©el des M√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation d'une s√©quence temporelle\n",
    "np.random.seed(42)\n",
    "\n",
    "n_frames = 300  # 10 secondes √† 30fps\n",
    "ear_values = []\n",
    "fatigue_states = []\n",
    "\n",
    "# Simulation: conducteur de plus en plus fatigu√©\n",
    "for i in range(n_frames):\n",
    "    if i < 100:\n",
    "        # √âveill√©\n",
    "        ear = np.random.normal(0.35, 0.03)\n",
    "        state = 'awake'\n",
    "    elif i < 200:\n",
    "        # Somnolent (clignements plus longs)\n",
    "        if 120 < i < 130 or 160 < i < 175:\n",
    "            ear = np.random.normal(0.15, 0.02)  # Yeux ferm√©s\n",
    "        else:\n",
    "            ear = np.random.normal(0.32, 0.04)\n",
    "        state = 'drowsy'\n",
    "    else:\n",
    "        # Tr√®s fatigu√© (micro-siestes)\n",
    "        if 210 < i < 230 or 250 < i < 280:\n",
    "            ear = np.random.normal(0.12, 0.02)  # Micro-sieste\n",
    "        else:\n",
    "            ear = np.random.normal(0.30, 0.05)\n",
    "        state = 'very_drowsy'\n",
    "    \n",
    "    ear_values.append(max(0.1, min(0.5, ear)))\n",
    "    fatigue_states.append(state)\n",
    "\n",
    "ear_values = np.array(ear_values)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# EAR au fil du temps\n",
    "axes[0].plot(ear_values, linewidth=1.5, color='blue', label='EAR')\n",
    "axes[0].axhline(y=0.25, color='r', linestyle='--', linewidth=2, label='Seuil (0.25)')\n",
    "axes[0].fill_between(range(n_frames), 0, 0.25, alpha=0.2, color='red', label='Zone fatigue')\n",
    "\n",
    "# Colorer les phases\n",
    "axes[0].axvspan(0, 100, alpha=0.1, color='green', label='Phase √©veill√©')\n",
    "axes[0].axvspan(100, 200, alpha=0.1, color='orange')\n",
    "axes[0].axvspan(200, 300, alpha=0.1, color='red')\n",
    "\n",
    "axes[0].set_ylabel('EAR', fontsize=12)\n",
    "axes[0].set_title('Simulation: √âvolution de l''EAR chez un conducteur fatigu√©', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0, 0.5)\n",
    "\n",
    "# Calcul du PERCLOS glissant\n",
    "window_size = 30\n",
    "perclos_values = []\n",
    "for i in range(n_frames):\n",
    "    start = max(0, i - window_size)\n",
    "    window = ear_values[start:i+1]\n",
    "    perclos = np.sum(window < 0.25) / len(window) if len(window) > 0 else 0\n",
    "    perclos_values.append(perclos)\n",
    "\n",
    "axes[1].plot(perclos_values, linewidth=1.5, color='purple', label='PERCLOS')\n",
    "axes[1].axhline(y=0.15, color='r', linestyle='--', linewidth=2, label='Seuil PERCLOS (15%)')\n",
    "axes[1].fill_between(range(n_frames), 0.15, 1, alpha=0.2, color='red')\n",
    "\n",
    "# Colorer les phases\n",
    "axes[1].axvspan(0, 100, alpha=0.1, color='green')\n",
    "axes[1].axvspan(100, 200, alpha=0.1, color='orange')\n",
    "axes[1].axvspan(200, 300, alpha=0.1, color='red')\n",
    "\n",
    "axes[1].set_xlabel('Frames (30 fps)', fontsize=12)\n",
    "axes[1].set_ylabel('PERCLOS', fontsize=12)\n",
    "axes[1].set_title('PERCLOS (Pourcentage de fermeture des yeux)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Annotations des phases\n",
    "axes[1].text(50, 0.8, '√âVEILL√â', fontsize=12, fontweight='bold', \n",
    "            color='green', ha='center')\n",
    "axes[1].text(150, 0.8, 'SOMNOLENT', fontsize=12, fontweight='bold', \n",
    "            color='orange', ha='center')\n",
    "axes[1].text(250, 0.8, 'DANGER', fontsize=12, fontweight='bold', \n",
    "            color='red', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/ear_timeseries.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pr√©paration des Donn√©es pour l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Division Train/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pr√©traitement de toutes les images\n",
    "print(\"Pr√©traitement des donn√©es...\")\n",
    "X_processed = []\n",
    "for img in tqdm(X_eyes, desc=\"Pr√©traitement\"):\n",
    "    processed = preprocessor.preprocess_eye(img)\n",
    "    X_processed.append(processed)\n",
    "\n",
    "X_processed = np.array(X_processed)\n",
    "\n",
    "# Division train/val/test\n",
    "# D'abord: train (70%) et temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_processed, y_eyes, \n",
    "    test_size=0.30, \n",
    "    random_state=42, \n",
    "    stratify=y_eyes\n",
    ")\n",
    "\n",
    "# Ensuite: val (15%) et test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"\\nDivision des donn√©es:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train: {X_train.shape[0]} √©chantillons ({X_train.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"Validation: {X_val.shape[0]} √©chantillons ({X_val.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} √©chantillons ({X_test.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"\\nForme des donn√©es: {X_train.shape}\")\n",
    "\n",
    "# Distribution des classes\n",
    "print(\"\\nDistribution des classes:\")\n",
    "print(f\"Train: {np.bincount(y_train)}\")\n",
    "print(f\"Val: {np.bincount(y_val)}\")\n",
    "print(f\"Test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Configuration de l'augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "# Visualisation d'exemples augment√©s\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "\n",
    "sample = X_train[0:1]  # Un √©chantillon\n",
    "\n",
    "# Image originale\n",
    "axes[0, 0].imshow(sample[0].squeeze(), cmap='gray')\n",
    "axes[0, 0].set_title('Originale', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# G√©n√©rer des variations\n",
    "augmented_images = []\n",
    "aug_iter = datagen.flow(sample, batch_size=1)\n",
    "\n",
    "for i in range(14):\n",
    "    aug_img = next(aug_iter)[0]\n",
    "    augmented_images.append(aug_img)\n",
    "\n",
    "# Afficher toutes les images\n",
    "for idx, img in enumerate(augmented_images, 1):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    axes[row, col].imshow(img.squeeze(), cmap='gray')\n",
    "    axes[row, col].set_title(f'Augment√©e #{idx}', fontweight='bold')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle('Exemples de Data Augmentation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/data_augmentation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Sauvegarde des Donn√©es Pr√©par√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des donn√©es pr√©par√©es\n",
    "import os\n",
    "\n",
    "processed_dir = '../data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Sauvegarde\n",
    "np.save(f'{processed_dir}/X_train.npy', X_train)\n",
    "np.save(f'{processed_dir}/X_val.npy', X_val)\n",
    "np.save(f'{processed_dir}/X_test.npy', X_test)\n",
    "np.save(f'{processed_dir}/y_train.npy', y_train)\n",
    "np.save(f'{processed_dir}/y_val.npy', y_val)\n",
    "np.save(f'{processed_dir}/y_test.npy', y_test)\n",
    "\n",
    "print(\"‚úì Donn√©es sauvegard√©es:\")\n",
    "print(f\"  - {processed_dir}/X_train.npy ({X_train.shape})\")\n",
    "print(f\"  - {processed_dir}/X_val.npy ({X_val.shape})\")\n",
    "print(f\"  - {processed_dir}/X_test.npy ({X_test.shape})\")\n",
    "print(f\"  - Labels correspondants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. R√©sum√© et Prochaines √âtapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 R√©sum√© de l'Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© visuel\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Titre principal\n",
    "fig.suptitle('R√âSUM√â DE L''EXPLORATION DES DONN√âES', \n",
    "            fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "# 1. √âchantillons\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.imshow(X_eyes[0].squeeze(), cmap='gray')\n",
    "ax1.set_title('≈íil Ouvert', fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.imshow(X_eyes[closed_indices[0]].squeeze(), cmap='gray')\n",
    "ax2.set_title('≈íil Ferm√©', fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "# 2. Distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.pie(np.bincount(y_eyes), labels=['Ouvert', 'Ferm√©'], \n",
    "       colors=['#2ecc71', '#e74c3c'], autopct='%1.1f%%')\n",
    "ax3.set_title('Distribution', fontweight='bold')\n",
    "\n",
    "# 3. EAR au fil du temps\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "ax4.plot(ear_values[:200], linewidth=1.5, color='blue')\n",
    "ax4.axhline(y=0.25, color='r', linestyle='--', linewidth=2)\n",
    "ax4.fill_between(range(200), 0, 0.25, alpha=0.2, color='red')\n",
    "ax4.set_ylabel('EAR')\n",
    "ax4.set_title('Simulation EAR', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Split des donn√©es\n",
    "ax5 = fig.add_subplot(gs[2, :2])\n",
    "splits = ['Train', 'Validation', 'Test']\n",
    "sizes = [len(X_train), len(X_val), len(X_test)]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "bars = ax5.bar(splits, sizes, color=colors, edgecolor='black')\n",
    "ax5.set_ylabel('Nombre d''√©chantillons')\n",
    "ax5.set_title('Division Train/Validation/Test', fontweight='bold')\n",
    "for bar, size in zip(bars, sizes):\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "            f'{size}\\n({size/len(X_processed)*100:.0f}%)', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 5. Info\n",
    "ax6 = fig.add_subplot(gs[2, 2])\n",
    "ax6.axis('off')\n",
    "info_text = f\"\"\"\n",
    "üìä STATISTIQUES\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Total √©chantillons: {len(X_processed)}\n",
    "\n",
    "Dimensions images:\n",
    "  {X_processed.shape[1]} √ó {X_processed.shape[2]}\n",
    "\n",
    "Canaux: {X_processed.shape[3]}\n",
    "\n",
    "Normalisation: [0, 1]\n",
    "\n",
    "Classes: 2 (binaire)\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, info_text, fontsize=11, family='monospace',\n",
    "        verticalalignment='center', bbox=dict(boxstyle='round', \n",
    "        facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.savefig('../reports/figures/exploration_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Prochaines √âtapes\n",
    "\n",
    "Le notebook suivant (02_modelisation_cnn.ipynb) couvrira:\n",
    "\n",
    "1. **Construction du CNN** - Architecture du mod√®le\n",
    "2. **Entra√Ænement** - Descente de gradient, callbacks\n",
    "3. **√âvaluation** - M√©triques de performance\n",
    "4. **Transfer Learning** - Fine-tuning de MobileNetV2\n",
    "\n",
    "### Concepts du Cours Appliqu√©s\n",
    "\n",
    "- ‚úÖ **Chapitre 1**: Normalisation, pr√©traitement\n",
    "- ‚úÖ **Chapitre 2**: Pr√©paration des donn√©es, split train/val/test\n",
    "- ‚úÖ **Chapitre 3-4**: Concepts de CNN (√† venir)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook 01 termin√©!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
